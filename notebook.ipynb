{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image preprocessing (+ image augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the desired image size\n",
    "image_size = (256, 256)\n",
    "\n",
    "# Define a function to preprocess and augment your dataset\n",
    "def preprocess_image(image):\n",
    "    # Resize the image to the desired size\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    \n",
    "    # Normalize pixel values to the range [-1, 1]\n",
    "    image = (image - 127.5) / 127.5\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Create a data generator for your dataset with extended data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    "    horizontal_flip=True,      # Flip images horizontally\n",
    "    rotation_range=20,         # Rotate images by up to 20 degrees\n",
    "    width_shift_range=0.1,     # Shift images horizontally by up to 10% of the width\n",
    "    height_shift_range=0.1,    # Shift images vertically by up to 10% of the height\n",
    "    shear_range=0.2,           # Shear transformations\n",
    "    zoom_range=0.2,            # Zoom in/out\n",
    "    channel_shift_range=0.1,   # Random channel shifts\n",
    "    brightness_range=[0.7, 1.3]  # Adjust brightness\n",
    ")\n",
    "\n",
    "# Load your dataset (assuming it's organized in a directory structure)\n",
    "dataset_path = '/training_images'  # path of dataset\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "# Create a TensorFlow data generator\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,  # Since you're doing style transfer, you don't need class labels\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator/descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the style transfer generator with a pretrained VGG16 encoder\n",
    "def build_style_transfer_generator(input_shape, vgg_encoder, latent_dim):\n",
    "    # Freeze the VGG encoder layers to prevent them from being updated\n",
    "    vgg_encoder.trainable = False\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = tf.keras.Sequential([\n",
    "        # Convolutional layers with upsampling\n",
    "        tf.keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu'),\n",
    "        tf.keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu'),\n",
    "        tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', activation='relu'),\n",
    "        tf.keras.layers.Conv2DTranspose(3, (3, 3), strides=(2, 2), padding='same', activation='tanh')  # Output layer\n",
    "    ])\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_image = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encode the input image using the pretrained VGG model\n",
    "    encoded = vgg_encoder(input_image)\n",
    "    \n",
    "    # Decode to get the stylized output\n",
    "    stylized_output = decoder(encoded)\n",
    "    \n",
    "    # Create the generator model\n",
    "    generator = tf.keras.Model(inputs=input_image, outputs=stylized_output)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# Define the style evaluation discriminator\n",
    "def build_style_evaluation_discriminator(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Convolutional layers with LeakyReLU activations\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.2)),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1)  # Output layer for binary classification\n",
    "    ])\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_image = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Get the discriminator's evaluation\n",
    "    evaluation = model(input_image)\n",
    "    \n",
    "    # Create the discriminator model\n",
    "    discriminator = tf.keras.Model(inputs=input_image, outputs=evaluation)\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "# Define the input shape and latent dimension\n",
    "input_shape = (256, 256, 3)\n",
    "latent_dim = 100  # Adjust as needed\n",
    "\n",
    "# Create a VGG16 model for use as the encoder\n",
    "vgg_encoder = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "# Build the style transfer generator and discriminator\n",
    "generator = build_style_transfer_generator(input_shape, vgg_encoder, latent_dim)\n",
    "discriminator = build_style_evaluation_discriminator(input_shape)\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    ")\n",
    "\n",
    "# Print model summaries for reference\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
